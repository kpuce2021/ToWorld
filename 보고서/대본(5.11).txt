저희는 언어장애인을 위한 스마트스피커 연동 어플을 구현하려고 합니다.

데모 영상 부터 보여드리겠습니다

처음에 저희 앱에 로그인을 합니다
로그인을 하면 녹음과 정지버튼을 통해 음성 녹음을 수행하고 
녹음이 완료되면 m4a 파일이 Firebase Storage에 업로드 됩니다.
업로드 되면 Storage에 trigger가 작동하면서 Cloud Function이 자동으로 작동하여
들어온 음성에 대한 예측이 수행됩니다.

이번엔 저번에 지적을 받았던 음성 데이터 확보 방법에 대한 방안을 모색한 것입니다.
사용자 녹음에 부담을 줄여주기 위해서 하나의 음성에서 나온 스펙트로그램을 여러개로 파생하는 방법입니다.
스펙트로그램에 마스킹 처리를 하여, 나온 결과입니다.

구현점과 미구현에 대해서 발표하겠습니다.
현재 사용모드의 대부분의 기능은 구현이 되었고, CNN을 완성하여 올바른 문장을 찾아낼 수 있다면 스마트 스피커에 음성을 전달할 수 있을 것입니다.
사용모드는 클라우드 펑션을 통해서 Python 코드로 Class 예측이 진행되는데
trigger을 통해 작동하는 함수가 m4a를 wav로 변환 , wav를 스펙트로그램으로 변환, 그레이스케일화, 모델을 통한 클래스 예측입니다.

학습모드에서 파란색 부분이 아직 구현되지 않은 상태인데, 아직 Training할 CNN 모델이 완성되지 않은 상태이므로 CNN 모델이 완벽히 구현된 후
기능을 구현할 예정입니다

SpecAugment는 앞서 말씀드려서 설명은 넘어가고, 이도 역시 CNN모델이 완성되면 음성 데이터의 정확도나 손실률, overfitting을 줄일 수 있는
데이터 파생 방법이 있으면 시도할 예정입니다.(예를 들어 구간을 이동하거나 진폭을 임의로 늘리거나)


마지막으로 
현재 구현한 CNN 모델이고 현재 학습된 데이터에 대해서는 overfitting도 나지 않고 잘 예측을 하지만 새로운 음성이 들어오면 전혀 예측을 하지 
못하는 상태입니다.
이를 개선하기 위해서 박성호 교수님께 면담 요청을 드렸고, 내일 설계한 모델에  대한 피드백을 주신다고 하셔서 상담후에 보완하려고 합니다.


