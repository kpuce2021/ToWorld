
저희는 2개의 Board 데일리 스크럼과 스프린트 백로그로 구성이 되어있습니다.
스프린트 백로그는 방학 때와 마찬가지로 해야할 것을 Product Backlog 
2주 동안 진행해야할 내용을 Sprint Backlog에 진행중인 것을 Sprint에
완료한 작업을 Done에 놓았습니다
그리고 보시다 시피 라벨로 작업의 특성을 구분해 놓았고 네임태그를 달아 누가 작업을 맡고
진행중에 있는지를 명시하였습니다. 또한 알림으로 시간을 Checklist로 달성 정도를 퍼센트로 표현하였습니다.

다음은 Daily Scrum입니다. 이곳은 하루에 할당된 작업의 내용을 올리는 Board입니다. 필요한 파일이나 링크를
공유하고 하루를 시작하고 마무리할 때 comment를 통해 소통을하고 있습니다. comment는 오늘 달성한 것과
부족해 내일로 밀린 작업 등을 씁니다. 또한 Daily Scrum을 Github에 하루에 한번씩 push하고 있습니다. 

다음은 Github입니다. 
저희 github 메인 화면이고 주제, 목적, 시스템 설계의 전반적인 내용을 md파일에 업로드해 놓았습니다.
저희의 master branch이고요 현재 안드로이드 스튜디오의 코드가 commit & push 된
상태이고 파이썬의 업로드가 잘 되지 않아서 알아보고 있는 중입니다. 안드로이드 스튜디오 코드의 진행 상황은 
앱의 시작화면 -> 녹음 기능 -> 녹음된 것을 firebase storage에 저장하는 것까지 구현해 놓았습니다. 합치진
못하였고 개별적인 기능으로 구현해 놓았습니다.

파이썬 코드도 업로드는 못하였지만 API 신청을 통해 Google Assitant를 사용하는 것까지 구현하였습니다
딥러닝 알고리즘에 대한 조사와 학습이 완료된 뒤 구현을 위한 RNN 학습 Test를 해볼 예정입니다.

마지막으로 trello를 github에 매일 push한 내용입니다.

지금부터는 시스템 설계입니다.
ALOHA는 AppLication fOr Hearing ImpAired에서 따온 약자이고요
저희 앱 로고는 귀에서 파도를 타는 것을 형상화 한 것으로 저희 앱의 기능과 적합해서 선택했습니다
앱 개발은 안드로이드 스튜디오 최신버전인 4.1.1 version으로 개발할 것이고
안드로이드 OS는 4.4 Kit-Kat을 사용할 예정입니다.

언어 장애인의 언어 장애 정도는 다양하기 때문에 개인화 학습이 필수적입니다 그래서 녹음된 파일을 사용자 id별
FireBase Storage 버킷에 저장

저희 메인 화면을 구상해보았는데요
이곳에서 이제 학습 또는 명령이 일어납니다. 학습모드 toggle을 On 시키면 스마트 스피커의 응답이
없이 학습만 시킬수 있습니다. 학습은 RNN으로 이루어 질 것입니다.

사용모드는 Toggle을 Off 시키면 사용할 수 있습니다. 이때는 스마트 스피커와의 상호작용이 이루어 집니다

앱에서 녹음된 파일을 Firebase로 전송을합니다. 위의 그림은 현재 Storage에 멀티미디어 파일을
업로드 할 수 있는지 테스트를 해본 것이고 Android Studio에서 업로드가 되는 것을 확인했습니다.

전송된 파일을 학습을 위한 데이터로 전처리를 해주어야합니다. 전처리는 파일 변환, 잡음 제거 이렇게
두가지 처리가 요구가 됩니다. 

저희가 공공 인공지능 오픈 API Data 서비스 포털에서 학습을 위한 API를 신청하고 받았습니다. 4가지 종류의 데이터를
받았는데 먼저 사용할 데이터는 음성학습 모델입니다. 사무실 잡음, 자동차 잡음, 잡음 없음, 발음이 좋은 한국인, 
발음이 나쁜 외국인, 나이대에 따른 발음 총 82408개의 text 파일로 라벨링된 음성 데이터입니다.

저희가 구상중인 학습 방법입니다. 애초에 API로 얻은 음성 모델을 언어 장애인의 발음과 유사하도록 고의적으로
뭉게버리는 방법입니다. 그리고 이를 템플릿 데이터로 사용하는 방법입니다

두번 째론 그냥 음성 모델을 RNN을 통해 학습시키고 사용자의 피드백을 통해 지속적으로 받아 정확도를 올리는
방법입니다. 이것은 첫 번째 방법이 불가할 때 사용할 예정입니다.

다음은 한국어 BERT 모델이고 문장 형태를 검증할 때 사용하는 모델입니다.
각 단어의 형태소 관계를 분석하여 올바른 문맥을 구성하도록 사용하는 형태소 데이터이고 음성 모델에서 도출된
문장을 Google Assistant가 알아들을 수 있도록 올바른 문장으로 재조합하도록 알고리즘을 구성할 예정입니다

결국 이렇게 올바른 문장이 도출이 되면 Google Assistant에 text 형태로 전달을 할 예정입니다.

그리고 반환도 역시 text로 받아서 청각장애인 화면을 통해 스마트 스피커의 동작 내용을 볼 수 있도록 구성할 예정입니다.

 

