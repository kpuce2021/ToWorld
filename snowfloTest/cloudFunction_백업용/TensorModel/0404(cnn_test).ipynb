{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번함수\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == class_names\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "    return tf.image.resize(img, [img_height, img_width])\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "# 2번 함수\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = pathlib.Path('C:/Users/jaehee/.keras/datasets/voice')\n",
    "#data_dir = pathlib.Path('C:/Users/jaehee/.keras/datasets/raw_data')\n",
    "data_dir = pathlib.Path('D:/#2021_CAPSTONE/_DataSet/rm_temp_spec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "data_dir\n",
    "image_count = len(list(data_dir.glob('*/*.png')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 files belonging to 3 classes.\n",
      "Using 525 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "img_height = 288\n",
    "img_width = 432\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    #color_mode=\"grayscale\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 files belonging to 3 classes.\n",
      "Using 450 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    #color_mode=\"grayscale\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1050 images belonging to 3 classes.\n",
      "Found 450 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# from keras import models, layers\n",
    "# from keras import Input\n",
    "# from keras.models import Model, load_model\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras import optimizers, initializers, regularizers, metrics\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n",
    " \n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# #data_dir = pathlib.Path('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data')\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# train_dir = os.path.join('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data_resnet/train')\n",
    "# val_dir = os.path.join('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data_resnet/test')\n",
    " \n",
    " \n",
    " \n",
    "# train_generator = train_datagen.flow_from_directory(train_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n",
    "# val_generator = val_datagen.flow_from_directory(val_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.preprocessing.image.DirectoryIterator'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(train_generator))\n",
    "# print(type(train_ds))\n",
    "\n",
    "# print(type(train_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['morning', 'traffic', 'weather']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "num_classes = len(class_names)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 256, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (32, 128, 256, 3)\n",
      "Label:  [2 2 0 0 1 2 1 2 0 2 1 1 2 2 0 1 0 1 2 0 0 0 1 2 2 1 1 2 1 0 2 1]\n",
      "Image shape:  (32, 128, 256, 3)\n",
      "Label:  [0 1 2 0 0 2 2 0 0 0 0 2 0 2 0 0 0 1 2 1 0 1 2 1 1 2 1 2 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(2):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 128, 256, 3), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위랑 성능 비교해 봐야 함\n",
    "\n",
    "# 1번 함수 사용 \n",
    "# train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "# val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# 2번 함수 사용\n",
    "# train_ds = configure_for_performance(train_ds)\n",
    "# val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 몇 개 데이터 이미지 시각화 시켜서 확인\n",
    "\n",
    "# image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#     label = label_batch[i]\n",
    "#     plt.title(class_names[label])\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003921569 0.95598966\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))\n",
    "#print(image_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "#     layers.Conv2D(16, (3,3), padding='same', activation='relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Dropout(0.2),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(num_classes)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## 여서부터 CNN 시작 ############\n",
    "# model = Sequential() # Sequential 모델은 각 레이어에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택에 적합합니다\n",
    "\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(256,128,1))) # Conv2D: 필터 수, kernal_size: 필터 크기, input_shape= 입력층 (가로: 50, 세로: 50, 채널: 3) 모델에 적용\n",
    "#                                                                                       # zero paddding의 값은? (Filter Size - 1) / 2\n",
    "#                                                                                     # 굳이 알필요는 없지만 출력층의 weight의 개수는? ( Input Size + 2 * Padding - Filter Size ) / Stride + 1 ( 4 + 2 * 0 - 2 ) / 1 + 1 = 3 * 3\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2))) # 사이즈 줄이고-> 여기서 padding을 설정하면 same으로 하면 같이 유지가 돼 가장 네모 정사각형 안겹치게 해서 가장 큰 값 뽑아내기\n",
    "# model.add(Dropout(0.25)) # Dropout란? 과적합을 방지하기 위해서 학습 시에 지정된 비율만큼 임의의 입력 뉴런(1차원)을 제외시킵니다.\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu')) # 첫번째 인자 : 출력 뉴런의 수를 설정합니다, input_dim : 입력 뉴런의 수를 설정합니다\n",
    "#                                          # init : 가중치 초기화 방법 설정합니다.‘uniform’ : 균일 분포, ‘normal’ : 가우시안 분포\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(3, activation='softmax')) # 소프트맥스 출력의 각 원소는 0.0 이상 1.0 이하의 실수입니다. 그리고 노드의 출력을 모두 합한 값이 항상 1이 됩니다.\n",
    "#                                           # 소프트맥스 함수의 좋은 점은 예측이 잘 이루어지면 1에 가까운 출력은 하나만 있고 다른 출력은 0에 가까워진다는 점입니다.\n",
    "#                                           # 하지만 예측이 잘 이루어지지 않으면 여러 레이블이 비슷한 확률을 가지게 될 수 있습니다.\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# history = model.fit_generator(train_generator, steps_per_epoch=200, epochs=50, validation_data=test_generator, validation_steps= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_9 (Rescaling)      (None, 128, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 128, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 64, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 128, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 16, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 4,218,403\n",
      "Trainable params: 4,218,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 128, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 134, 262, 3)  0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 128, 64)  9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 128, 64)  256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 128, 64)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 66, 130, 64)  0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 64, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 64, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 64, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 64, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 64, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 64, 256)  0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 64, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 64, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 64, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 64, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 64, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 64, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 64, 256)  0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 64, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 64, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 64, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 64, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 64, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 64, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 64, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 64, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 64, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 64, 256)  0           batch_normalization_10[0][0]     \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 64, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 32, 128)  32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 32, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 32, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 32, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 32, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 32, 512)  131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 32, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 32, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 32, 512)  0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 32, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 32, 128)  65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 32, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 32, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 32, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 32, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 32, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 32, 512)  66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 32, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 32, 512)  0           batch_normalization_17[0][0]     \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 32, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 32, 128)  65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 32, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 32, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 32, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 32, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 32, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 32, 512)  2048        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 32, 512)  0           batch_normalization_20[0][0]     \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 32, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 32, 128)  65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 32, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 32, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 32, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 32, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 32, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 32, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 32, 512)  0           batch_normalization_23[0][0]     \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 32, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 16, 256)   131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 16, 256)   1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 16, 256)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 16, 256)   590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 16, 256)   1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 16, 256)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 16, 1024)  263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 16, 1024)  525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 16, 1024)  4096        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 16, 1024)  4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 16, 1024)  0           batch_normalization_26[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 16, 1024)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 16, 256)   262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 16, 256)   1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 16, 256)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 16, 256)   590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 16, 256)   1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 16, 256)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 16, 1024)  263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 16, 1024)  4096        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 16, 1024)  0           batch_normalization_30[0][0]     \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 16, 1024)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 16, 256)   262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 16, 256)   1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 16, 256)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 16, 256)   590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 16, 256)   1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 16, 256)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 16, 1024)  263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 16, 1024)  4096        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 16, 1024)  0           batch_normalization_33[0][0]     \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 16, 1024)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 16, 256)   262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 16, 256)   1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 16, 256)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 16, 256)   590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 16, 256)   1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 16, 256)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 16, 1024)  263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 16, 1024)  4096        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 16, 1024)  0           batch_normalization_36[0][0]     \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 16, 1024)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 16, 256)   262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 16, 256)   1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 16, 256)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 16, 256)   590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 16, 256)   1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 16, 256)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 16, 1024)  263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 16, 1024)  4096        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 16, 1024)  0           batch_normalization_39[0][0]     \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 16, 1024)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 16, 256)   262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 16, 256)   1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 16, 256)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 16, 256)   590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 16, 256)   1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 16, 256)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 16, 1024)  263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 16, 1024)  4096        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 16, 1024)  0           batch_normalization_42[0][0]     \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 16, 1024)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 8, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 8, 512)    2048        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 8, 512)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 8, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 8, 512)    2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 8, 512)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 8, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 8, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 8, 2048)   8192        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 8, 2048)   8192        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 8, 2048)   0           batch_normalization_45[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 8, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 8, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 8, 512)    2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 8, 512)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 8, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 8, 512)    2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 8, 512)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 8, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 8, 2048)   8192        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 8, 2048)   0           batch_normalization_49[0][0]     \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 8, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 8, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 8, 512)    2048        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 8, 512)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 8, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 8, 512)    2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 8, 512)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 8, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 8, 2048)   8192        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 8, 2048)   0           batch_normalization_52[0][0]     \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 8, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            6147        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 23,593,859\n",
      "Trainable params: 23,540,739\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "from keras import Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers, initializers, regularizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n",
    " \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#data_dir = pathlib.Path('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data')\n",
    "\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#val_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "#train_dir = os.path.join('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data/train')\n",
    "#val_dir = os.path.join('C:/Users/jaehee/.keras/datasets/final_log_mel_spec_data/test')\n",
    " \n",
    " \n",
    " \n",
    "#train_generator = train_datagen.flow_from_directory(train_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n",
    "#val_generator = val_datagen.flow_from_directory(val_dir, batch_size=16, target_size=(224, 224), color_mode='rgb')\n",
    "\n",
    "# number of classes\n",
    "#K = 4\n",
    "K = 3\n",
    " \n",
    "input_tensor = Input(shape=(128, 256, 3), dtype='float32', name='input') # shape=(224, 224, 3)\n",
    "\n",
    "def conv1_layer(x):    \n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    " \n",
    "    return x   \n",
    " \n",
    "    \n",
    "def conv2_layer(x):         \n",
    "    x = MaxPooling2D((3, 3), 2)(x)     \n",
    " \n",
    "    shortcut = x\n",
    " \n",
    "    for i in range(3):\n",
    "        if (i == 0):\n",
    "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(shortcut)            \n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    " \n",
    "            x = Add()([x, shortcut])\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            shortcut = x\n",
    " \n",
    "        else:\n",
    "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])   \n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            shortcut = x        \n",
    "    \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv3_layer(x):        \n",
    "    shortcut = x    \n",
    "    \n",
    "    for i in range(4):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(128, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)            \n",
    " \n",
    "            x = Add()([x, shortcut])    \n",
    "            x = Activation('relu')(x)    \n",
    " \n",
    "            shortcut = x              \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(128, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])     \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x      \n",
    "            \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv4_layer(x):\n",
    "    shortcut = x        \n",
    "  \n",
    "    for i in range(6):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(256, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(1024, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    " \n",
    "            x = Add()([x, shortcut]) \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x               \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)            \n",
    " \n",
    "            x = Add()([x, shortcut])    \n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            shortcut = x      \n",
    " \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "def conv5_layer(x):\n",
    "    shortcut = x    \n",
    "  \n",
    "    for i in range(3):     \n",
    "        if(i == 0):            \n",
    "            x = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)        \n",
    "            \n",
    "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)  \n",
    " \n",
    "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            shortcut = Conv2D(2048, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
    "            x = BatchNormalization()(x)\n",
    "            shortcut = BatchNormalization()(shortcut)            \n",
    " \n",
    "            x = Add()([x, shortcut])  \n",
    "            x = Activation('relu')(x)      \n",
    " \n",
    "            shortcut = x               \n",
    "        \n",
    "        else:\n",
    "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            \n",
    "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
    "            x = BatchNormalization()(x)           \n",
    "            \n",
    "            x = Add()([x, shortcut]) \n",
    "            x = Activation('relu')(x)       \n",
    " \n",
    "            shortcut = x                  \n",
    " \n",
    "    return x\n",
    " \n",
    " \n",
    " \n",
    "x = conv1_layer(input_tensor)\n",
    "x = conv2_layer(x)\n",
    "x = conv3_layer(x)\n",
    "x = conv4_layer(x)\n",
    "x = conv5_layer(x)\n",
    " \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output_tensor = Dense(K, activation='softmax')(x)\n",
    " \n",
    "resnet50 = Model(input_tensor, output_tensor)\n",
    "resnet50.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오래걸리는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "33/33 [==============================] - 43s 736ms/step - loss: 1.3849 - accuracy: 0.6796 - val_loss: 3907.3486 - val_accuracy: 0.3422\n",
      "Epoch 2/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 0.1906 - accuracy: 0.9416 - val_loss: 118.5562 - val_accuracy: 0.3422\n",
      "Epoch 3/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.0812 - accuracy: 0.9649 - val_loss: 24.7442 - val_accuracy: 0.3467\n",
      "Epoch 4/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 0.0666 - accuracy: 0.9729 - val_loss: 15.7507 - val_accuracy: 0.3511\n",
      "Epoch 5/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.0468 - accuracy: 0.9851 - val_loss: 0.2312 - val_accuracy: 0.9311\n",
      "Epoch 6/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.0420 - accuracy: 0.9839 - val_loss: 1.3112 - val_accuracy: 0.6644\n",
      "Epoch 7/300\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 0.0358 - accuracy: 0.9868 - val_loss: 0.3107 - val_accuracy: 0.9200\n",
      "Epoch 8/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 1.8176 - val_accuracy: 0.4756\n",
      "Epoch 9/300\n",
      "33/33 [==============================] - 12s 378ms/step - loss: 0.0539 - accuracy: 0.9812 - val_loss: 1.2498 - val_accuracy: 0.7467\n",
      "Epoch 10/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 0.0287 - accuracy: 0.9848 - val_loss: 1.2046 - val_accuracy: 0.6511\n",
      "Epoch 11/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.8500 - val_accuracy: 0.6622\n",
      "Epoch 12/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 0.0904 - accuracy: 0.9796 - val_loss: 63.5497 - val_accuracy: 0.3467\n",
      "Epoch 13/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.1424 - accuracy: 0.9657 - val_loss: 73.9960 - val_accuracy: 0.3644\n",
      "Epoch 14/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 0.0760 - accuracy: 0.9725 - val_loss: 1.8437 - val_accuracy: 0.6556\n",
      "Epoch 15/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.5333 - val_accuracy: 0.8022\n",
      "Epoch 16/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.1769 - val_accuracy: 0.9400\n",
      "Epoch 17/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 0.0257 - accuracy: 0.9919 - val_loss: 0.2720 - val_accuracy: 0.9444\n",
      "Epoch 18/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0273 - val_accuracy: 0.9889\n",
      "Epoch 19/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.0313 - val_accuracy: 0.9911\n",
      "Epoch 20/300\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 1.0064 - val_accuracy: 0.8044\n",
      "Epoch 21/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 0.0116 - accuracy: 0.9986 - val_loss: 0.0429 - val_accuracy: 0.9911\n",
      "Epoch 22/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9911\n",
      "Epoch 23/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 5.1583e-04 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9956\n",
      "Epoch 24/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 2.6825e-04 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9956\n",
      "Epoch 25/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.0301e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9956\n",
      "Epoch 26/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 8.8227e-05 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9956\n",
      "Epoch 27/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 8.8047e-05 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 0.9956\n",
      "Epoch 28/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 5.4650e-05 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9956\n",
      "Epoch 29/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 5.9620e-05 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9956\n",
      "Epoch 30/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 4.9634e-05 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9956\n",
      "Epoch 31/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 5.5811e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 32/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 4.6332e-05 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 33/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 4.5054e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 34/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.5213e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 35/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.1497e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 36/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.8787e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 37/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.3921e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 38/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.0231e-05 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 39/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.0369e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 40/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 2.2421e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 41/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.9511e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 42/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.0505e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 43/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 2.2071e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 44/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.2719e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 45/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.8166e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 46/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.6253e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 47/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.6565e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 48/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.4534e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 49/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.2633e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 50/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.5384e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 51/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 1.2718e-05 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 52/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 1.4628e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 53/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.1617e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 54/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.0746e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 55/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.1249e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 56/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.0492e-05 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 57/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 9.4570e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 58/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.0844e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 59/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 9.7092e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 60/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 7.8765e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 61/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.0594e-05 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 62/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 7.0216e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 63/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 8.0380e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 64/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 8.7140e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 65/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 7.1340e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 66/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 8.2600e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 67/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.3879e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 68/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 7.1613e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 69/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.3006e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 70/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 7.2809e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 71/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 5.1374e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 72/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 7.0822e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 73/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 5.2373e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 74/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.2481e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 75/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 4.7575e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 76/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 4.7127e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 77/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 5.8974e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 78/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 5.2521e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 79/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 5.1355e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 80/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 4.4411e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 81/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 4.8807e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 82/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 4.5776e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 83/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 4.2396e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 84/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 3.7525e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 85/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 5.2123e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 86/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 5.3610e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 87/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 3.9795e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 88/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 3.8740e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 89/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 3.7928e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 90/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 3.2900e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 91/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.4580e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 92/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.3861e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 93/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.3464e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 94/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 3.3239e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 95/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 3.0137e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 96/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.7760e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 97/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 2.8392e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 98/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.1441e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 99/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.0745e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 100/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.2944e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 101/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.8069e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 102/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.2460e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 103/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.3378e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 104/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.6407e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 105/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.2315e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 106/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.6144e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 107/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.3623e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 108/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.1095e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 109/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.3546e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 110/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.9063e-06 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.1200e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 112/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.1199e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 113/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.5189e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 114/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.3495e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 115/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.0636e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 116/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.1617e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 117/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.8694e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 118/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.8876e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 119/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.8433e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 120/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.8490e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 121/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.7762e-06 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 122/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.4148e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 123/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.5970e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 124/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.2970e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 125/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.6621e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 126/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.3479e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 127/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.1578e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 128/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.6535e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 129/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.1511e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 130/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.4775e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 131/300\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 1.2407e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 132/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.1004e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 133/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 1.0831e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 134/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 1.2734e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 135/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 9.2933e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 136/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 1.1169e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 137/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 9.3206e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 138/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 9.5719e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 139/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.2668e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 140/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.0980e-06 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 141/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.0804e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 142/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 9.5620e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 143/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 9.4152e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 144/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 1.3251e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 145/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 8.2459e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 146/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 9.0202e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 147/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 1.2720e-06 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 148/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 9.0142e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 149/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 8.9686e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 150/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 7.8056e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 151/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 9.6344e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 152/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 8.3413e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 153/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 8.9516e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 154/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 8.6750e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 155/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 9.4703e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 156/300\n",
      "33/33 [==============================] - 12s 359ms/step - loss: 7.4401e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 157/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 7.7002e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 158/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 8.3041e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 159/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 7.0834e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 160/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 7.8833e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 161/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 6.4123e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 162/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 6.8931e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 163/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 6.5230e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 164/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 6.6963e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 165/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 5.8953e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 166/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.7223e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 167/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.1993e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 168/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 5.3876e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 169/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 5.8774e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 170/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 6.0909e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 171/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 6.9738e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 172/300\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 4.6968e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 173/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 4.4445e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 174/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 4.9897e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 175/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 5.0596e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 176/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 4.8732e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 177/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 5.2312e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 178/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 5.3093e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 179/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 5.1304e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 180/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 5.6579e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 181/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 4.5691e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 182/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 4.2201e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 183/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 4.5876e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 184/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 4.7195e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 185/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 3.6911e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 186/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 4.9214e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 187/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 3.6551e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 188/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 4.9875e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 189/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 3.2403e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 190/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 4.1325e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 191/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 4.3003e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 192/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.1914e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 193/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.2050e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 194/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.0548e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 195/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.7113e-07 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9956\n",
      "Epoch 196/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 3.2922e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 197/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.1269e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 198/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.7044e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 199/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.7930e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 200/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.9513e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 201/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 3.1171e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 202/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 3.3471e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 203/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 2.3694e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 204/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 2.5121e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 205/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 2.6910e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 206/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 3.2550e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 207/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 2.5527e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 208/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 2.4392e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 209/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 2.2003e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 210/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 2.0800e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 211/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 2.4403e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 212/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 2.8302e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 213/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 2.9354e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 214/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 2.6524e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 215/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 2.9015e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 216/300\n",
      "33/33 [==============================] - 12s 361ms/step - loss: 1.7543e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 217/300\n",
      "33/33 [==============================] - 12s 362ms/step - loss: 2.3054e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 218/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.8529e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 219/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 1.7541e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 2.1059e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 221/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 1.9943e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 222/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 2.3074e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 223/300\n",
      "33/33 [==============================] - 12s 360ms/step - loss: 1.8379e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 224/300\n",
      "33/33 [==============================] - 12s 363ms/step - loss: 2.1923e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 225/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 2.0979e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 226/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 2.3201e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 227/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 1.9862e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 228/300\n",
      "33/33 [==============================] - 12s 364ms/step - loss: 1.6134e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 229/300\n",
      "33/33 [==============================] - 12s 375ms/step - loss: 1.7845e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 230/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 2.0964e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 231/300\n",
      "33/33 [==============================] - 12s 375ms/step - loss: 1.3677e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 232/300\n",
      "33/33 [==============================] - 12s 376ms/step - loss: 2.1423e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 233/300\n",
      "33/33 [==============================] - 12s 376ms/step - loss: 1.7680e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 234/300\n",
      "33/33 [==============================] - 13s 386ms/step - loss: 2.0985e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 235/300\n",
      "33/33 [==============================] - 12s 375ms/step - loss: 1.6859e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 236/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.8534e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 237/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.4923e-07 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9956\n",
      "Epoch 238/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.3372e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 239/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.1533e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 240/300\n",
      "33/33 [==============================] - 12s 376ms/step - loss: 1.5234e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 241/300\n",
      "33/33 [==============================] - 12s 376ms/step - loss: 1.2513e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 242/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.5213e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 243/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.4684e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 244/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.3005e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 245/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 1.1404e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 246/300\n",
      "33/33 [==============================] - 13s 382ms/step - loss: 1.2828e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 247/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 1.3129e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 248/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.3761e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 249/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.3908e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 250/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.4909e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 251/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.2600e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 252/300\n",
      "33/33 [==============================] - 12s 378ms/step - loss: 9.3076e-08 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 253/300\n",
      "33/33 [==============================] - 13s 401ms/step - loss: 1.1180e-07 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9956\n",
      "Epoch 254/300\n",
      "33/33 [==============================] - 14s 417ms/step - loss: 8.7986e-08 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 255/300\n",
      "33/33 [==============================] - 14s 421ms/step - loss: 8.5253e-08 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 256/300\n",
      "33/33 [==============================] - 14s 425ms/step - loss: 1.0331e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 257/300\n",
      "33/33 [==============================] - 14s 439ms/step - loss: 1.0674e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 258/300\n",
      "33/33 [==============================] - 15s 453ms/step - loss: 1.1018e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 259/300\n",
      "33/33 [==============================] - 13s 398ms/step - loss: 1.0810e-07 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 260/300\n",
      "33/33 [==============================] - 13s 384ms/step - loss: 8.2511e-08 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 261/300\n",
      "33/33 [==============================] - 12s 373ms/step - loss: 1.0454e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 262/300\n",
      "33/33 [==============================] - 12s 376ms/step - loss: 9.3875e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 263/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 7.5461e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 264/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 1.2383e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 265/300\n",
      "33/33 [==============================] - 12s 371ms/step - loss: 8.8085e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 266/300\n",
      "33/33 [==============================] - 12s 371ms/step - loss: 8.9640e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 267/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 8.5582e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 268/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.2567e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 269/300\n",
      "33/33 [==============================] - 13s 398ms/step - loss: 7.2099e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 270/300\n",
      "33/33 [==============================] - 13s 385ms/step - loss: 9.3635e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 271/300\n",
      "33/33 [==============================] - 13s 382ms/step - loss: 1.1372e-07 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 272/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 7.5153e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 273/300\n",
      "33/33 [==============================] - 12s 381ms/step - loss: 9.1572e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 274/300\n",
      "33/33 [==============================] - 13s 381ms/step - loss: 5.8086e-08 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 275/300\n",
      "33/33 [==============================] - 13s 382ms/step - loss: 7.7566e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 276/300\n",
      "33/33 [==============================] - 13s 403ms/step - loss: 8.5113e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 277/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 8.3221e-08 - accuracy: 1.0000 - val_loss: 0.0224 - val_accuracy: 0.9956\n",
      "Epoch 278/300\n",
      "33/33 [==============================] - 13s 384ms/step - loss: 7.6219e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 279/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 1.0006e-07 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 280/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 8.3276e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 281/300\n",
      "33/33 [==============================] - 12s 372ms/step - loss: 6.3311e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 282/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 7.1183e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 283/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 6.2258e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 284/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 7.0839e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 285/300\n",
      "33/33 [==============================] - 12s 366ms/step - loss: 6.2981e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 286/300\n",
      "33/33 [==============================] - 12s 365ms/step - loss: 6.4275e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 287/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 7.4865e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 288/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 5.1864e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 289/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 6.3585e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 290/300\n",
      "33/33 [==============================] - 12s 371ms/step - loss: 5.9516e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 291/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 5.4528e-08 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9956\n",
      "Epoch 292/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 4.1847e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 293/300\n",
      "33/33 [==============================] - 12s 374ms/step - loss: 6.3711e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 294/300\n",
      "33/33 [==============================] - 12s 369ms/step - loss: 6.0002e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 295/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 5.2525e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 296/300\n",
      "33/33 [==============================] - 12s 368ms/step - loss: 5.6811e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 297/300\n",
      "33/33 [==============================] - 12s 370ms/step - loss: 4.9366e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n",
      "Epoch 298/300\n",
      "33/33 [==============================] - 13s 395ms/step - loss: 4.6590e-08 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9956\n",
      "Epoch 299/300\n",
      "33/33 [==============================] - 12s 375ms/step - loss: 5.2229e-08 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9956\n",
      "Epoch 300/300\n",
      "33/33 [==============================] - 12s 375ms/step - loss: 5.7619e-08 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    EPOCHS=300\n",
    "    history = resnet50.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        #callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "resnet50.save('resnet50_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = resnet50.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file :\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_avg = 0.9983\n",
      "test_avg = 0.9734\n"
     ]
    }
   ],
   "source": [
    "train_avg = np.mean(history.history['accuracy'])\n",
    "test_avg = np.mean(history.history['val_accuracy'])\n",
    "print('train_avg = {0:.4f}'.format(train_avg))\n",
    "print('test_avg = {0:.4f}'.format(test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "plt.savefig('Epoch  300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ab16224fc56e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'voice_predict_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save('voice_predict_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-654bbba046ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# plt.imshow(gray, cmap='gray')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mimgGray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimgGray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_gray.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2911\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2912\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# image = cv2.imread('C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png')\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "# plt.imshow(gray, cmap='gray')\n",
    "# plt.show()\n",
    "img = Image.open('C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png')\n",
    "imgGray = img.convert('L')\n",
    "imgGray.save('test_gray.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "image_path = 'C:\\\\Users\\\\jaehee\\\\Desktop\\\\jupyter_proj\\\\test_voice\\\\tvon.png'\n",
    "img = image.load_img(image_path, target_size=(img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 128, 256, 1) for input Tensor(\"rescaling_12_input:0\", shape=(None, 128, 256, 1), dtype=float32), but it was called on an input with incompatible shape (32, 1, 256, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer conv2d_21 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [32, 1, 256, 3]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-6d0bab07c4b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Create a batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m           expand_composites=True)\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3141\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3142\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\jaehee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer conv2d_21 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [32, 1, 256, 3]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "image_path = 'C:/Users/jaehee/.keras/datasets/new_gray_data/all/living_room5.png'\n",
    "img = image.load_img(image_path, target_size=(img_height, img_width))\n",
    "\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 1) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "print(predictions)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "print(score)\n",
    "print(\n",
    "    \"새로운 데이터는 {} 클래스일 확률이 {:.2f}%입니다..\".format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('voice_predict_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_3 (Rescaling)      (None, 128, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 128, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 128, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 4,218,403\n",
      "Trainable params: 4,218,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone2021",
   "language": "python",
   "name": "capstone2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
